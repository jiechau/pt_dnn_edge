<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ONNX.js Inference</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"></script>
</head>
<body>
    <h1>ONNX.js Inference Example</h1>
    <div id="output"></div>
    
    <script>
        async function runInference() {
            // Initialize ONNX model session
            const session = new onnx.InferenceSession();
            await session.loadModel('./model.onnx');
    
            // Prepare input data (example data)
            const watermelonQty = 1.0;
            const appleQty = 2.0;
            const grapeQty = 3.0;
            const inputData = new Float32Array([watermelonQty, appleQty, grapeQty]);
    
            // Create an input tensor
            const inputTensor = new onnx.Tensor(inputData, 'float32', [1, 3]);
    
            // Run inference
            const outputMap = await session.run({ input: inputTensor });
            const outputTensor = outputMap.values().next().value;
    
            // Display the output
            document.getElementById('output').innerText = `Output: ${outputTensor.data}`;
        }
    
        // Run the inference when the page loads
        window.onload = runInference;
    </script>
    
</body>
</html>
